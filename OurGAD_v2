import time
import numpy as np
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc
import math
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.nn.parameter import Parameter
from torch.nn.modules.module import Module
# from OurGAD import SinkhornDistance,target_distribution_sampling,GraphConvolution,PagerankDiffusion,TransformerEncoder,Projector,DotProductDecoder,Recover
 
# Adapted from https://github.com/gpeyre/SinkhornAutoDiff
class SinkhornDistance(nn.Module):
    r"""
    Given two empirical measures each with :math:`P_1` locations
    :math:`x\in\mathbb{R}^{D_1}` and :math:`P_2` locations :math:`y\in\mathbb{R}^{D_2}`,
    outputs an approximation of the regularized OT cost for point clouds.
    Args:
        eps (float): regularization coefficient
        max_iter (int): maximum number of Sinkhorn iterations
        reduction (string, optional): Specifies the reduction to apply to the output:
            'none' | 'mean' | 'sum'. 'none': no reduction will be applied,
            'mean': the sum of the output will be divided by the number of
            elements in the output, 'sum': the output will be summed. Default: 'none'
    Shape:
        - Input: :math:`(N, P_1, D_1)`, :math:`(N, P_2, D_2)`
        - Output: :math:`(N)` or :math:`()`, depending on `reduction`
    """
    def __init__(self, eps=0.1, max_iter=2000, thresh=1e-3, reduction='none', device='cpu'):
        super(SinkhornDistance, self).__init__()
        self.eps = eps
        self.max_iter = max_iter
        self.thresh = thresh
        self.reduction = reduction
        self.device = device
        print('=============== SinkHorn ===============')
        print(f'========= epsilon:{self.eps}')
        print(f'========= max iteration:{self.max_iter}')
        print(f'========= stop threshold:{self.thresh}')
        print('=============== SinkHorn ===============')

    def forward(self, x, y, normalized=False):
        # The Sinkhorn algorithm takes as input three variables :
        C = self._cost_matrix(x, y, normalized=normalized)  # Wasserstein cost function
        x_points = x.shape[-2]
        y_points = y.shape[-2]
        if x.dim() == 2:
            batch_size = 1
        else:
            batch_size = x.shape[0]

        # both marginals are fixed with equal weights
        mu = torch.empty(batch_size, x_points, dtype=torch.float,
                         requires_grad=False).fill_(1.0 / x_points).squeeze()
        nu = torch.empty(batch_size, y_points, dtype=torch.float,
                         requires_grad=False).fill_(1.0 / y_points).squeeze()

        u = torch.zeros_like(mu).to(self.device)
        v = torch.zeros_like(nu).to(self.device)
        # To check if algorithm terminates because of threshold
        # or max iterations reached
        actual_nits = 0
        # Stopping criterion
        thresh = self.thresh
        # thresh = 1e-3

        # Sinkhorn iterations
        for i in range(self.max_iter):
            u1 = u  # useful to check the update
            u = self.eps * (torch.log(mu+1e-8).to(self.device) - torch.logsumexp(self.M(C, u, v).to(self.device), dim=-1)) + u
            v = self.eps * (torch.log(nu+1e-8).to(self.device) - torch.logsumexp(self.M(C, u, v).transpose(-2, -1).to(self.device), dim=-1)) + v
            err = (u - u1).abs().sum(-1).mean()

            actual_nits += 1
            if err.item() < thresh:
                # print(f'error:{err.item()}')
                break
        # if actual_nits == self.max_iter:
        #     print('meeting max iteration.')            
        U, V = u, v
        # Transport plan pi = diag(a)*K*diag(b)
        pi = torch.exp(self.M(C, U, V))
        # Sinkhorn distance
        cost = torch.sum(pi * C, dim=(-2, -1))

        if self.reduction == 'mean':
            cost = cost.mean()
        elif self.reduction == 'sum':
            cost = cost.sum()

        # return cost, pi, C
        return cost

    def M(self, C, u, v):
        "Modified cost for logarithmic updates"
        "$M_{ij} = (-c_{ij} + u_i + v_j) / \epsilon$"
        return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / self.eps

    @staticmethod
    def _cost_matrix(x, y, p=2, normalized=False):
        "Returns the matrix of $|x_i-y_j|^p$."
        x_col = x.unsqueeze(-2)
        y_lin = y.unsqueeze(-3)
        C = torch.sum((torch.abs(x_col - y_lin)) ** p, -1)
        if normalized:
            C = C / torch.norm(C)
        return C

    @staticmethod
    def ave(u, u1, tau):
        "Barycenter subroutine, used by kinetic acceleration through extrapolation."
        return tau * u + (1 - tau) * u1

def target_distribution_sampling(size, sample_dim, r_max=None, r_min=0.0, mu=0, std=1):
    '''
    Sample points from normal distribution with norm in [r_min, r_max]
    
    Args:
        size: number of samples to generate
        sample_dim: dimension of each sample
        r_max: maximum norm value
        r_min: minimum norm value
        mu: mean of normal distribution
        std: standard deviation of normal distribution
    
    Returns:
        Tensor of shape (size, sample_dim) with norm in [r_min, r_max]
    '''
    targets = []
    max_attempts = 1000  # Prevent infinite loops
    
    while len(targets) < size:
        attempts = 0
        while attempts < max_attempts:
            # Generate sample from normal distribution
            sample = randn(sample_dim, mean=mu, std=std)
            sample_norm = torch.norm(sample, p=2)
            
            # Scale sample to desired norm range
            if sample_norm > 0:
                # Generate random norm in [r_min, r_max]
                target_norm = torch.rand(1) * (r_max - r_min) + r_min
                scaled_sample = sample * (target_norm / sample_norm)
                
                if r_min <= torch.norm(scaled_sample) <= r_max:
                    targets.append(scaled_sample)
                    break
                    
            attempts += 1
            
        if attempts >= max_attempts:
            raise RuntimeError(f"Failed to generate sample with norm in [{r_min}, {r_max}] after {max_attempts} attempts")
    
    return torch.stack(targets)

def randn(sample_dim, mean=0.0, std=1.0):

    '''
    N(0, 1) Gaussian
    '''
    return torch.distributions.Normal(loc=mean, scale=std).sample(sample_dim).squeeze(-1)

class GraphConvolution(Module):
    """
    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907
    """

    def __init__(self, in_features, out_features, bias=True):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.FloatTensor(in_features, out_features))
        if bias:
            self.bias = Parameter(torch.FloatTensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, input, adj):
        support = torch.mm(input.float(), self.weight.float())
        output = torch.spmm(adj.float(), support)
        if self.bias is not None:
            return output + self.bias
        else:
            return output

    def __repr__(self):
        return self.__class__.__name__ + ' (' \
               + str(self.in_features) + ' -> ' \
               + str(self.out_features) + ')'

class GCN(nn.Module):
    def __init__(self, nfeat, nhid, nclass, dropout, tau=0.5):
        super(GCN, self).__init__()

        self.gc1 = GraphConvolution(nfeat, nhid)
        self.gc2 = GraphConvolution(nhid, nclass)
        self.dropout = dropout
        self.tau = tau

    def forward(self, adj, x):
        h = self.gc1(x, adj)
        h = F.relu(h)
        h = F.dropout(h, self.dropout, training=self.training)
        h = self.gc2(h, adj)

        return h
 
class PagerankDiffusion(nn.Module):
    def __init__(self, alpha=0.85, niter=50):
        super().__init__()
        self.alpha = alpha
        self.niter = niter
        
    def forward(self, adj):
        # Compute personalized PageRank
        deg = adj.sum(1)
        deg_inv_sqrt = torch.pow(deg, -0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        norm_adj = deg_inv_sqrt.unsqueeze(1) * adj * deg_inv_sqrt.unsqueeze(0)
        
        # Diffusion process
        pr = torch.eye(adj.size(0)).to(adj.device)
        for _ in range(self.niter):
            pr = self.alpha * torch.mm(norm_adj, pr) + (1 - self.alpha) * torch.eye(adj.size(0)).to(adj.device)
        return pr

class TransformerEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_heads=None):
        super().__init__()
        if num_heads is None:
            # Try common num_heads values (1-10) that divide input_dim
            possible_heads = [n for n in [8,6,4,3,2,1] if input_dim % n == 0]
            num_heads = possible_heads[0] if possible_heads else 1
            print(f"Auto-selected num_heads={num_heads} for input_dim={input_dim}")
        self.self_attn = nn.MultiheadAttention(input_dim, num_heads)
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.linear2 = nn.Linear(hidden_dim, input_dim)
        self.norm1 = nn.LayerNorm(input_dim)
        self.norm2 = nn.LayerNorm(input_dim)
        
    def forward(self, x):
        attn_output, _ = self.self_attn(x, x, x)
        x = self.norm1(x + attn_output)
        ff_output = self.linear2(F.relu(self.linear1(x)))
        x = self.norm2(x + ff_output)
        return x

class Projector(nn.Module):

    def __init__(self, input_dim, mid_dim):
        super(Projector, self).__init__()
        self.input_dim = input_dim
        self.mid_dim = mid_dim

        self.project = GraphConvolution(input_dim, mid_dim)

    def forward(self, x, adj):

        return self.project(x, adj)

class DotProductDecoder(nn.Module):
    r"""
    Simple dot product decoder for structure reconstruction, which is
    defined as :math:`\symbf{A}' = \sigma(\symbf{X}
    \symbf{X}^\intercal)`, where :math:`\sigma` is the optional sigmoid
    function, :math:`\symbf{X}` is the input node features, and the
    :math:`\symbf{A}'` is the reconstructed adjacency matrix.

    Parameters
    ----------
    sigmoid_s : bool, optional
        Whether to apply sigmoid to the structure reconstruction.
        Default: ``False``.
    """

    def __init__(self, sigmoid_s=False):
        super(DotProductDecoder, self).__init__()
        self.sigmoid_s = sigmoid_s

    def forward(self, x, edge_index=None):
        """
        Forward computation.

        Parameters
        ----------
        x : torch.Tensor
            Input node features.
        edge_index : torch.Tensor, optional
            Edge index (not used, kept for compatibility).

        Returns
        -------
        s_ : torch.Tensor
            Reconstructed adjacency matrix.
        """
        s_ = x @ x.T
        if self.sigmoid_s:
            s_ = torch.sigmoid(s_)
        return s_

class Recover(nn.Module):

    def __init__(self, mid_dim, recover_dim) -> None:
        super(Recover, self).__init__()
        self.mid_dim = mid_dim
        self.recover_dim = recover_dim

        self.decoder = GraphConvolution(mid_dim, recover_dim)

    def forward(self, x, adj):
        
        return self.decoder(x, adj)
  
class GCNDecoder(nn.Module):
    def __init__(self, num_nodes, in_size, hidden_size, out_size):
        super(GCNDecoder, self).__init__()
        self.tgc1 = GraphConvolution(in_size, hidden_size)
        self.tgc2 = GraphConvolution(hidden_size, out_size)
        self.dropout = 0.5
        self.linear = nn.Linear(num_nodes, in_size, bias=True)
        self.pe_feat = torch.FloatTensor(torch.eye(num_nodes))#.to(device)

    def forward(self, adj, indices):
        pe = self.linear(self.pe_feat[indices])
        # pe = F.dropout(pe, self.dropout, training=self.training)
        h = self.tgc1(pe, adj)
        h = F.relu(h)
        h = F.dropout(h, self.dropout, training=self.training)
        h = self.tgc2(h, adj)

        return h
    
class MVNet(nn.Module):
    """
    重构为双路径模型：
    1、特征重构路径：
        1.1、使用TransformerEncoder进行特征编码得到feat_recon
        1.2、使用DotProductDecoder解码为struct_recon
        1.3、使用Projector投影到潜在空间mid_repre
        1.4、使用Recover恢复成原数据
    2、结构重构路径：
        2.1、使用PagerankDiffusion进行结构编码得到struct_recon 
        2.2、使用GCN生成feat_recon
        2.3、使用Projector投影到潜在空间mid_repre
        2.4、使用Recover恢复成原数据
    graph TD
      A[输入数据] --> B[特征路: TransformerEncoder]
      A --> C[结构路: PagerankDiffusion]
      
      B --> D[DotProductDecoder]
      D --> E[Projector]
      E --> F[Recover]
      F --> G[重构数据]
      
      C --> H[GCNDecoder]
      H --> I[Projector]
      I --> J[Recover]
      J --> G
      
      E -.潜在空间对比.-> I
      D -.结构对比.-> C
      B -.特征对比.-> H
    """

    def __init__(self, num_nodes, input_dim, mid_dim):
        super(MVNet, self).__init__()
        # 特征重构路径组件
        self.feat_encoder = TransformerEncoder(input_dim, mid_dim, num_heads=None)
        self.feat_decoder = DotProductDecoder()
        self.feat_projector = Projector(input_dim=input_dim, mid_dim=mid_dim)
        self.feat_recover = Recover(mid_dim=mid_dim, recover_dim=input_dim)
        
        # 结构重构路径组件
        self.struct_encoder = PagerankDiffusion()
        self.struct_decoder = GCNDecoder(num_nodes, input_dim, mid_dim, input_dim)
        self.struct_projector = Projector(input_dim=input_dim, mid_dim=mid_dim)
        self.struct_recover = Recover(mid_dim=mid_dim, recover_dim=input_dim)

    def pretrain_forward(self, x, adj, indices):
        """预训练阶段前向传播"""
        # 特征重构路径
        feat_recon = self.feat_encoder(x)
        struct_recon_feat = self.feat_decoder(feat_recon)
        mid_repre_feat = self.feat_projector(feat_recon, struct_recon_feat)
        recover_data_feat = self.feat_recover(mid_repre_feat, struct_recon_feat)
        
        # 结构重构路径
        struct_recon_struct = self.struct_encoder(adj)
        feat_recon_struct = self.struct_decoder(struct_recon_struct, indices)
        mid_repre_struct = self.struct_projector(feat_recon_struct, struct_recon_struct)
        recover_data_struct = self.struct_recover(mid_repre_struct, struct_recon_struct)

        return {
            'feat': {
                'feat_recon': feat_recon,
                'struct_recon': struct_recon_feat,
                'mid_repre': mid_repre_feat,
                'recover_data': recover_data_feat
            },
            'struct': {
                'feat_recon': feat_recon_struct,
                'struct_recon': struct_recon_struct,
                'mid_repre': mid_repre_struct,
                'recover_data': recover_data_struct
            }
        }

    def generate_pseudo_anomalies(self, num_nodes, mid_dim, r_min, r_max):
        """在潜在空间生成伪异常样本
        
        Args:
            num_nodes: 要生成的样本数量
            mid_dim: 潜在空间维度
            r_min: 最小半径
            r_max: 最大半径
            
        Returns:
            生成的伪异常样本张量
        """
        # 生成约10%数量的伪异常样本
        num_samples = max(1, int(num_nodes * 0.1))
        
        # 使用更大的半径范围生成异常样本
        pseudo_anomalies = target_distribution_sampling(
            num_samples,
            (mid_dim,),  # 潜在空间维度
            r_min=r_max * 1.2,  # 从正常样本范围外开始
            r_max=r_max * 2.0,  # 更大的半径范围
            mu=0.0,
            std=0.5
        )
        return pseudo_anomalies

    def generate_pseudo_graph(self, pseudo_anomalies, k=5):
        """基于特征相似度生成伪异常图结构
        
        Args:
            pseudo_anomalies: 伪异常特征矩阵 (num_nodes, feature_dim)
            k: 近邻数
            
        Returns:
            邻接矩阵 (num_nodes, num_nodes)
        """
        from sklearn.neighbors import kneighbors_graph
        import scipy.sparse
        
        # 转换为CPU numpy数组
        features_np = pseudo_anomalies.cpu().numpy()
        
        # 构建k近邻图
        adj = kneighbors_graph(features_np, 
                             n_neighbors=k,
                             mode='connectivity',
                             metric='cosine',
                             include_self=False)
        
        # 转换为对称矩阵
        adj = adj.maximum(adj.T)
        
        # 转换为PyTorch张量
        adj = scipy.sparse.coo_matrix(adj)
        indices = torch.LongTensor(np.vstack((adj.row, adj.col)))
        values = torch.FloatTensor(adj.data)
        adj = torch.sparse_coo_tensor(indices, values, 
                                    torch.Size(adj.shape),
                                    dtype=torch.float32,
                                    device=pseudo_anomalies.device).to_dense()
        
        # 添加自环
        adj += torch.eye(adj.size(0), device=pseudo_anomalies.device)
        
        return adj.to(pseudo_anomalies.device)

    def finetune_forward(self, pseudo_anomalies, adj=None, missing_rate=0.3):
        """微调阶段前向传播"""
        # 如果没有提供邻接矩阵，则生成一个
        if adj is None:
            adj = self.generate_pseudo_graph(pseudo_anomalies)

        # TODO:是否recover去掉adj部分,因为这个的adj是基于潜在空间下pseudo_anomalies生成的,可以预训练阶段的不是一致的,是不是分布上有问题
        # 通过特征路,先恢复伪异常样本
        recovered_data_feat = self.feat_recover(pseudo_anomalies, adj)

        # 通过结构路,先恢复伪异常样本
        recovered_data_struct = self.struct_recover(pseudo_anomalies, adj)

        # 模拟缺失值
        rand_vals = torch.rand(recovered_data_feat.size())
        mask = rand_vals <= missing_rate
        recovered_data_feat[mask] = 0.0
        recovered_data_struct[mask] = 0.0
            
        # 通过特征路,再进行特征重构
        feat_recon = self.feat_encoder(recovered_data_feat)
        struct_recon_feat = self.feat_decoder(feat_recon)
        mid_repre_feat = self.feat_projector(feat_recon, struct_recon_feat)
        
        # TODO:结构路,这里有问题,因为他是基于真实数据分布下的结构来重构的
        # 通过结构路,再进行结构重构
        struct_recon_struct = self.struct_encoder(adj)
        feat_recon_struct = None # self.struct_decoder(struct_recon_struct)   # 这里是因为伪造的节点,没有再PE中有确切位置
        mid_repre_struct = None # self.struct_projector(feat_recon_struct, struct_recon_struct)

        return {
            'feat': {
                'feat_recon': feat_recon,
                'struct_recon': struct_recon_feat,
                'mid_repre': mid_repre_feat,
                'recover_data': recovered_data_feat
            },
            'struct': {
                'feat_recon': feat_recon_struct,
                'struct_recon': struct_recon_struct,
                'mid_repre': mid_repre_struct,
                'recover_data': recovered_data_struct
            },
            'mask': mask
        }
        
class GAD:
    def __init__(self, args, adj, features, ano_label, masks):
        self.args = args
        self.device = self.args.device
        self.optimizer_name = self.args.optimizer_name
        self.lr= self.args.lr
        self.beta = self.args.beta
        self.r_min = self.args.r_min
        self.r_max = self.args.r_max
        self.batch_size = self.args.batch_size if self.args.use_batch else None  # None means process all data at once
        self._lambda = self.args._lambda
        self.entropy_reg_coe = self.args.entropy_reg_coe
        self.stop_threshold = self.args.stop_threshold

        # Early stopping parameters
        self.patience = 10  # Number of epochs to wait before stopping
        self.best_loss = float('inf')
        self.counter = 0
        self.early_stop = False

        self.adj = adj
        self.features = features
        self.labels = ano_label
        self.masks = masks

        # 计算两个概率分布距离
        self.sinkhorn_loss = SinkhornDistance(eps=self.entropy_reg_coe, max_iter=int(1e3), thresh=self.stop_threshold, device=self.device)

        # 构建网络
        self.net = MVNet(
            num_nodes=self.features.shape[0],
            input_dim=self.features.shape[1], 
            mid_dim=args.mid_dim
        )

        # Set device for network
        self.net = self.net.to(self.device)
        # Set optimizer
        if self.optimizer_name == 'adam':
            self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)
        elif self.optimizer_name == 'sgd':
            self.optimizer = optim.SGD(self.net.parameters(), lr=self.lr)
        else:
            raise Exception(f'Unknown optimizer name [{self.optimizer_name}].')

    def train(self):        
        # 预训练阶段
        print("Starting pretraining phase...")
        self.net.train()
        pretrain_loss = self._train_phase('pretrain')
        
        # 预训练后就进行一次测试
        auc = self.test()
        print('Test set pretrain AUROC: {:.4f}'.format(100. * auc))

        # 生成伪异常样本
        print("Generating pseudo anomalies...")
        pseudo_anomalies = self.net.generate_pseudo_anomalies(self.features.shape[0], self.args.mid_dim, self.r_min, self.r_max)
        # pseudo_anomalies = pseudo_anomalies.to(self.device)

        # 微调阶段
        print("Starting finetuning phase...")
        self.net.train()
        finetune_loss = self._train_phase('finetune', pseudo_anomalies)
        
        return min(pretrain_loss, finetune_loss)

    def _train_phase(self, phase, pseudo_anomalies=None):
        best_loss = float('inf')
        counter = 0
        early_stop = False
        
        for epoch in range(self.args.epochs):
            t = time.time()
            self.optimizer.zero_grad()
            
            # 初始化损失
            total_loss = 0
            
            if phase == 'pretrain':
                # 预训练阶段使用正常样本
                indices = torch.nonzero(self.labels == 0).squeeze()
                num_nodes = len(indices)
                
                # 分批处理
                if self.batch_size is not None and num_nodes > self.batch_size:
                    num_batches = math.ceil(num_nodes / self.batch_size)
                    
                    for batch_idx in range(num_batches):
                        print(f'-------------------------------{batch_idx}---------------------------')
                        start = batch_idx * self.batch_size
                        end = min((batch_idx + 1) * self.batch_size, num_nodes)
                        batch_indices = indices[start:end]
                        
                        # 获取batch数据
                        batch_features = self.features[batch_indices]
                        batch_masks = self.masks[batch_indices]
                        
                        # 处理邻接矩阵
                        if not isinstance(self.adj, torch.Tensor):
                            adj_csr = self.adj.tocsr()
                            batch_indices_np = batch_indices.cpu().numpy()
                            batch_rows = adj_csr[batch_indices_np].todense()
                            batch_adj = torch.from_numpy(batch_rows[:, batch_indices_np]).to(self.device)
                        else:
                            batch_adj = self.adj[batch_indices][:, batch_indices]
                        
                        # 前向传播
                        outputs = self.net.pretrain_forward(batch_features, batch_adj, batch_indices)
                        
                        ####################################特征重构路############################################
                        # 计算特征重构路径损失
                        feat_recon_loss = torch.mean(torch.sum(torch.mul(
                            (outputs['feat']['recover_data'] - batch_features), 
                            batch_masks) ** 2,
                            dim=tuple(range(1, batch_features.dim())))) * self._lambda
                        current_loss = feat_recon_loss

                        # 计算特征重构路径潜在空间分布loss
                        targets = target_distribution_sampling(
                            batch_features.shape[0], 
                            outputs['feat']['mid_repre'][0].shape,
                            r_min=self.r_min, r_max=self.r_max, mu=0.0, std=0.5)
                        dist_loss = self.sinkhorn_loss(outputs['feat']['mid_repre'], targets)
                        current_loss += dist_loss

                        ####################################结构重构路############################################
                        # 计算结构重构路径损失
                        struct_contrast_loss = torch.mean(torch.sum(torch.mul(
                            (outputs['struct']['recover_data'] - batch_features), 
                            batch_masks) ** 2,
                            dim=tuple(range(1, batch_features.dim())))) * self._lambda
                        current_loss += struct_contrast_loss

                        # 计算结构重构路径潜在空间分布loss
                        targets = target_distribution_sampling(
                            batch_features.shape[0], 
                            outputs['struct']['mid_repre'][0].shape,
                            r_min=self.r_min, r_max=self.r_max, mu=0.0, std=0.5)
                        targets = targets.to(self.device)
                        dist_loss = self.sinkhorn_loss(outputs['struct']['mid_repre'], targets)
                        current_loss += dist_loss
                        
                        ####################################特征和结构############################################                        
                        # 计算两路特征重构对比loss
                        feat_contrast_loss = F.mse_loss(
                            outputs['feat']['feat_recon'],
                            outputs['struct']['feat_recon']) * self.beta
                        current_loss += feat_contrast_loss
                        
                        # 计算两路结构重构对比loss
                        struct_contrast_loss = F.mse_loss(
                            outputs['feat']['struct_recon'],
                            outputs['struct']['struct_recon']) * self.beta
                        current_loss += struct_contrast_loss
                        
                        # 累加batch损失（保持计算图）
                        if batch_idx == 0:
                            batch_loss = current_loss
                        else:
                            batch_loss += current_loss
                        
                    # 计算平均损失
                    batch_loss = batch_loss / num_batches
                    total_loss = batch_loss.item()
                else:
                    # 处理整个数据集
                    inputs = self.features[indices]
                    masks = self.masks[indices]
                    
                    # 处理邻接矩阵
                    if not isinstance(self.adj, torch.Tensor):
                        adj_csr = self.adj.tocsr()
                        indices_np = indices.cpu().numpy()
                        rows = adj_csr[indices_np].todense()
                        adj = torch.from_numpy(rows[:, indices_np]).to(self.device)
                    else:
                        adj = self.adj[indices][:, indices]
                    
                    # 前向传播
                    outputs = self.net.pretrain_forward(inputs, adj, indices)
                    
                    ####################################特征重构路############################################
                    # 计算特征重构路径损失
                    feat_recon_loss = torch.mean(torch.sum(torch.mul(
                        (outputs['feat']['recover_data'] - inputs), 
                        masks) ** 2,
                        dim=tuple(range(1, inputs.dim())))) * self._lambda
                    current_loss = feat_recon_loss

                    # 计算特征重构路径潜在空间分布loss
                    targets = target_distribution_sampling(
                        inputs.shape[0], 
                        outputs['feat']['mid_repre'][0].shape,
                        r_min=self.r_min, r_max=self.r_max, mu=0.0, std=0.5)
                    targets = targets.to(self.device)
                    dist_loss = self.sinkhorn_loss(outputs['feat']['mid_repre'], targets)
                    current_loss += dist_loss

                    ####################################结构重构路############################################
                    # 计算结构重构路径损失
                    struct_contrast_loss = torch.mean(torch.sum(torch.mul(
                        (outputs['struct']['recover_data'] - inputs), 
                        masks) ** 2,
                        dim=tuple(range(1, inputs.dim())))) * self._lambda
                    current_loss += struct_contrast_loss

                    # 计算结构重构路径潜在空间分布loss
                    targets = target_distribution_sampling(
                        inputs.shape[0], 
                        outputs['struct']['mid_repre'][0].shape,
                        r_min=self.r_min, r_max=self.r_max, mu=0.0, std=0.5)
                    targets = targets.to(self.device)
                    dist_loss = self.sinkhorn_loss(outputs['struct']['mid_repre'], targets)
                    current_loss += dist_loss
                    
                    ####################################特征和结构############################################                        
                    # 计算两路特征重构对比loss
                    feat_contrast_loss = F.mse_loss(
                        outputs['feat']['feat_recon'],
                        outputs['struct']['feat_recon']) * self.beta
                    current_loss += feat_contrast_loss
                    
                    # 计算两路结构重构对比loss
                    struct_contrast_loss = F.mse_loss(
                        outputs['feat']['struct_recon'],
                        outputs['struct']['struct_recon']) * self.beta
                    current_loss += struct_contrast_loss
                    batch_loss = current_loss
                    total_loss = batch_loss.item()
                
            elif phase == 'finetune':
                # 微调阶段处理伪异常样本
                if epoch == 0:
                    pseudo_adj = self.net.generate_pseudo_graph(pseudo_anomalies)
                
                # 前向传播
                outputs = self.net.finetune_forward(pseudo_anomalies, pseudo_adj)

                ####################################特征重构路############################################
                # 计算特征重构路径潜在空间分布loss
                dist_loss = self.sinkhorn_loss(outputs['feat']['mid_repre'], pseudo_anomalies)
                current_loss = dist_loss

                ####################################结构重构路############################################
                # 计算结构重构路径潜在空间分布loss
                # dist_loss = self.sinkhorn_loss(outputs['struct']['mid_repre'], pseudo_anomalies)
                # current_loss += dist_loss
                
                ####################################特征和结构############################################                        
                # 计算两路特征重构对比loss
                # feat_contrast_loss = F.mse_loss(
                #     outputs['feat']['feat_recon'],
                #     outputs['struct']['feat_recon']) * self.beta
                # current_loss += feat_contrast_loss
                
                # 计算两路结构重构对比loss
                struct_contrast_loss = F.mse_loss(
                    outputs['feat']['struct_recon'],
                    outputs['struct']['struct_recon']) * self.beta
                current_loss += struct_contrast_loss
                batch_loss = current_loss
                total_loss = batch_loss.item()
            
            # 反向传播和优化
            batch_loss.backward()
            self.optimizer.step()
            
            print(f'{phase} epoch: {epoch+1:04d}',
                  f'total loss: {total_loss:.4f}',
                  f'time: {time.time() - t:.4f}s')
            
            # Early stopping 逻辑
            if total_loss < best_loss:
                best_loss = total_loss
                counter = 0
            else:
                counter += 1
                if counter >= self.patience:
                    early_stop = True
                    print(f'Early stopping triggered at {phase} epoch {epoch+1}')
            
            if early_stop:
                break
                
        return best_loss

    def test(self):
        self.net.eval()
        all_scores = []
        with torch.no_grad():
            num_nodes = self.features.shape[0]
            
            if self.batch_size is None:
                # Process all data at once
                batch_indices = torch.arange(num_nodes)
                batch_features = self.features[batch_indices]
                
                # Handle sparse adj matrix
                if not isinstance(self.adj, torch.Tensor):
                    adj_csr = self.adj.tocsr()
                    batch_indices_np = batch_indices.cpu().numpy()
                    batch_rows = adj_csr[batch_indices_np].todense()
                    batch_adj = torch.from_numpy(batch_rows[:, batch_indices_np]).to(self.device)
                else:
                    batch_adj = self.adj[batch_indices][:, batch_indices]
                
                # Forward pass
                outputs = self.net.pretrain_forward(batch_features, batch_adj, batch_indices)
                
                # 计算特征路径异常分数
                feat_scores = torch.sqrt(torch.sum(outputs['feat']['mid_repre'] ** 2, 
                                                dim=tuple(range(1, outputs['feat']['mid_repre'].dim()))))
                
                # 计算结构路径异常分数
                struct_scores = torch.sqrt(torch.sum(outputs['struct']['mid_repre'] ** 2,
                                                  dim=tuple(range(1, outputs['struct']['mid_repre'].dim()))))
                
                # 综合两路分数
                scores = (feat_scores + struct_scores) / 2
                all_scores.append(scores)
            else:
                # Process in batches
                for batch_start in range(0, num_nodes, self.batch_size):
                    batch_end = min(batch_start + self.batch_size, num_nodes)
                    batch_indices = torch.arange(batch_start, batch_end)
                    
                    # Get batch features
                    batch_features = self.features[batch_indices]
                    
                    # Handle sparse adj matrix for batch
                    if not isinstance(self.adj, torch.Tensor):
                        adj_csr = self.adj.tocsr()
                        batch_indices_np = batch_indices.cpu().numpy()
                        batch_rows = adj_csr[batch_indices_np].todense()
                        batch_adj = torch.from_numpy(batch_rows[:, batch_indices_np]).to(self.device)
                    else:
                        batch_adj = self.adj[batch_indices][:, batch_indices]
                    
                    # Forward pass
                    outputs = self.net.pretrain_forward(batch_features, batch_adj, batch_indices)
                    
                    # 计算特征路径异常分数
                    feat_scores = torch.sqrt(torch.sum(outputs['feat']['mid_repre'] ** 2, 
                                                    dim=tuple(range(1, outputs['feat']['mid_repre'].dim()))))
                    
                    # 计算结构路径异常分数
                    struct_scores = torch.sqrt(torch.sum(outputs['struct']['mid_repre'] ** 2,
                                                      dim=tuple(range(1, outputs['struct']['mid_repre'].dim()))))
                    
                    # 综合两路分数
                    batch_scores = (feat_scores + struct_scores) / 2
                    all_scores.append(batch_scores)
            
            # Combine all batch scores
            scores = torch.cat(all_scores).detach().cpu().numpy()
            auroc = roc_auc_score(self.labels, scores)
        
        return auroc
    

# Training settings
parser = argparse.ArgumentParser() # 20241101bmwsf132513#dev
parser.add_argument('--name', type=str, default="testrun", help='Provide a test name.')

parser.add_argument('--epochs', type=int, default=500, help='Number of epochs for NET')
parser.add_argument('--mid_dim', type=int, default=128, help='Hidden size')
parser.add_argument('--optimizer_name', default='adam')
parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.')
parser.add_argument('--_lambda', type=float, default=1.0, help='Initial learning rate.')
parser.add_argument('--alpha', type=float, default=1.0, help='Initial learning rate.')
parser.add_argument('--beta', type=float, default=1.0, help='Initial learning rate.')
parser.add_argument('--stop_threshold', type=float, default=1e-4, help='Initial learning rate.')
parser.add_argument('--entropy_reg_coe', type=float, default=0.1, help='Initial learning rate.')
parser.add_argument('--batch_size', type=int, default=1000, help='Batch size for processing.')
parser.add_argument('--use_batch', default=True, action='store_true', help='Enable batch processing.')
parser.add_argument('--r_min', type=float, default=0.0, help='Minimum radius for target distribution.')   # 8.45
parser.add_argument('--r_max', type=float, default=8.45, help='Initial learning rate.')  # 16.9
parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate (1 - keep probability).')

def run_ourgad(dataset, adj, features, ano_label, mask):
    args = parser.parse_args()
    args.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    args.name = args.name + '_' + time.strftime('%d_%m_%Y') + '_' + time.strftime('%H:%M:%S')
    args.dataset = dataset
    # if not os.path.exists('.checkpoints'):
    #     os.makedirs('.checkpoints')

    # 特征标准化
    features[~mask] = (features[~mask] - np.mean(features[~mask])) / (np.std(features[~mask]) + 1e-8)
    features[mask] = 0
    
    mask_tensor = torch.from_numpy(mask)
    features = torch.from_numpy(features)
    
    # 优化邻接矩阵生成
    # if not isinstance(adj, torch.Tensor):
    #     adj = adj.tocoo()
    #     indices = torch.LongTensor(np.vstack((adj.row, adj.col)))
    #     values = torch.FloatTensor(adj.data)
    #     adj = torch.sparse_coo_tensor(indices, values, adj.shape).to_dense()
    # else:
    #     adj = adj.clone()   # 算法中是否会对adj修改
    
    ano_label = torch.from_numpy(ano_label)

    gad = GAD(args, adj, features, ano_label, mask_tensor)
    gad.train()
    auc = gad.test()
    print('Test set finetune AUROC: {:.4f}'.format(100. * auc))

    auc = np.round(auc, 4) 
    return auc

from data import load_data_and_mask

adj, features, labels, idx_train, idx_val, idx_test, ano_label, str_ano_label, attr_ano_label, mask = load_data_and_mask('disney', 0.3, 'cpu')
auc_avg = run_ourgad('disney',adj,features,ano_label,mask)
print(auc_avg)
